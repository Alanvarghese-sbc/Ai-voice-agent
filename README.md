<!-- # ğŸ™ï¸ AI Voice Agent â€” 30 Days of AI Voice Agents (Day 13)

## ğŸ“Œ Project Overview
This project is part of the **30 Days of AI Voice Agents** challenge.  
Itâ€™s a **fully functional conversational AI agent** that listens to voice input, processes it using an LLM, and responds back in natural speech â€” all while remembering context from previous conversations.

---

## ğŸ› ï¸ Technologies Used
- **Frontend:** HTML, CSS, JavaScript
- **Backend:** Python, FastAPI
- **AI Services:**
  - Speech-to-Text (STT) â€” Converts voice input to text
  - Large Language Model (LLM) â€” Generates intelligent responses
  - Text-to-Speech (TTS) â€” Converts AI responses back into speech
- **Datastore:** In-memory / custom datastore for chat history

---

## âš™ï¸ Architecture
1. **Voice Input** â†’ User speaks into the microphone.
2. **STT Service** â†’ Audio is transcribed into text.
3. **LLM API** â†’ Processes user query along with previous chat history.
4. **TTS Service** â†’ AI-generated text response is converted to audio.
5. **UI Playback** â†’ Audio response is played back to the user.
6. **Loop** â†’ Conversation continues with remembered context.

---

## âœ¨ Features
âœ… Real-time voice-to-voice AI conversation  
âœ… Chat history to maintain conversation context  
âœ… Error handling with fallback responses  
âœ… Fully responsive, interactive UI  
âœ… Single-button recording system with animations  

---

## ğŸ“¸ Screenshots
*(Add screenshots here of your UI and terminal responses)*

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/YOUR_USERNAME/YOUR_REPO_NAME.git
cd YOUR_REPO_NAME -->

# AI Voice Agent 

## Overview
This project is part of the **30 Days of AI Voice Agents** challenge.  
It is a **web-based voice assistant** that allows users to interact with an AI through voice. The application can:

- Transcribe your speech in real-time.
- Generate intelligent conversational responses using AI.
- Convert the AI's text responses into natural-sounding audio.
- Provide a seamless, interactive UI for recording, playback, and continuous conversation.

This challenge helped me explore **voice interfaces, AI APIs, and modern frontend-backend integration**.

---

## Key Features

1. **Voice-to-Text (Speech Recognition)**  
   - Users can speak to the agent.  
   - Speech is converted to text using AssemblyAI API.  

2. **Conversational AI Response**  
   - Text from user speech is sent to Gemini AI (or other LLM) for generating context-aware responses.  

3. **Text-to-Speech (TTS)**  
   - AI responses are converted to audio using Murf API.  
   - Fallback audio is played if TTS fails.  

4. **Interactive UI**  
   - Start/Stop Recording using a single toggle button.  
   - Audio playback starts automatically once the AI responds.  
   - Responsive and modern UI with animations for recording state.  

5. **Session Management**  
   - Each page load creates a unique session ID.  
   - Allows multi-turn conversations with context persistence.

---

## Technologies Used

- **Frontend:** HTML, CSS, JavaScript  
- **Backend:** Python, FastAPI  
- **APIs & Services:**  
  - **Murf API** â€“ Text-to-Speech  
  - **AssemblyAI API** â€“ Speech-to-Text  
  - **Gemini API** â€“ Conversational AI  
- **Other Tools:**  
  - dotenv â€“ Environment variable management  
  - MediaRecorder API â€“ Browser-side audio recording  

---

## Project Architecture
```bash

User (Browser)
â”‚
â”œâ”€> Record Audio / Input Text
â”‚
Frontend (HTML, CSS, JS)
â”‚
â”œâ”€> API Request to FastAPI
â”‚
Backend (FastAPI)
â”‚ â”œâ”€> AssemblyAI â†’ Transcribe speech to text
â”‚ â”œâ”€> Gemini â†’ Generate AI response
â”‚ â””â”€> Murf â†’ Convert AI response to speech
â”‚
â””â”€> Return transcription & audio URL to frontend

```


---

## Setup & Installation

### 1. Clone the repository
```bash
git clone https://github.com/<username>/<repo-name>.git
cd <repo-name>
```

### 2. Create a virtual environment
```bash
python -m venv venv
# Activate virtual environment
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```


### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Create .env file in project root
```bash
MURF_API_KEY=your_murf_api_key
ASSEMBLY_API_KEY=your_assemblyai_api_key
GEMINI_API_KEY=your_gemini_api_key
```

### 5. Run the FastAPI server
```bash
uvicorn app:app --reload
```

### 6. Open your browser and visit
```bash
 http://localhost:8000. 

```
---

## ğŸ“‚ Project Structure
```bash
30-days-of-voice-agents
â”œâ”€â”€ day-13/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â”œâ”€â”€ script.js
â”‚   â”‚   â””â”€â”€ fallback.mp3
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env
... (other day folders)


```

<!-- Screenshots

Include screenshots of your updated UI. -->

<!-- Capture recording button animations and audio playback. -->
---
## Usage

âœ…Click the ğŸ™ï¸ Start Recording button to speak to the AI.

âœ…Stop recording by clicking the â¹ï¸ Stop Recording button.

âœ…The AI will transcribe your speech, generate a response, and play audio automatically.

âœ…Your conversation history is displayed in the transcript box.

---
